{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import  StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer, MinMaxScaler\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, GaussianNoise, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical \n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data function for reading and processing the train and test sets\n",
    "#necessary as an input for the optimisation algorithm\n",
    "def data():\n",
    "    #define input processing function\n",
    "    def process_attributes(df, train, test):\n",
    "        \n",
    "        #define and fit the scaler to the full dataset\n",
    "        cs = MinMaxScaler()\n",
    "        cs.fit(df_inputs.select_dtypes(np.number))\n",
    "        \n",
    "        #scale the numerical input variables\n",
    "        trainContinuous = cs.transform(train.select_dtypes(np.number))\n",
    "        testContinuous = cs.transform(test.select_dtypes(np.number))\n",
    "        \n",
    "        #uncomment the code below to accommodate for any categorical columns\n",
    "        zipBinarizer = LabelBinarizer().fit(df[\"Gender\"])\n",
    "        trainCategorical = zipBinarizer.transform(train[\"Gender\"])\n",
    "        testCategorical = zipBinarizer.transform(test[\"Gender\"])\n",
    "        \n",
    "        # construct our training and testing data points by concatenating\n",
    "        # the categorical features with the continuous features\n",
    "        trainX = np.hstack([trainContinuous, trainCategorical])\n",
    "        testX = np.hstack([testContinuous, testCategorical])\n",
    "        \n",
    "        #return the processed train and test sets\n",
    "        #trainX=trainContinuous\n",
    "        #testX=testContinuous\n",
    "        \n",
    "        # return the concatenated training and testing data\n",
    "        return (trainX, testX)\n",
    "    \n",
    "    #read the excel datasets\n",
    "    df = pd.read_excel('Cleaned_Dataframe.xlsx')\n",
    "    df.set_index('Sample',inplace=True)\n",
    "\n",
    "\n",
    "    #separate cancer markers and input data\n",
    "    df_outputs= df['Status']\n",
    "    df_inputs = df.drop('Status',axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_inputs, df_outputs, random_state=100, stratify=df_outputs, test_size=0.3)\n",
    "    \n",
    "    #process the input sets\n",
    "    (X_train_sc, X_test_sc) = process_attributes(df_inputs, X_train, X_test)\n",
    "    \n",
    "    #encode the categorical output variables\n",
    "    #encode categorical outputs\n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(y_train)\n",
    "    Y_train= lb.transform(y_train)\n",
    "    Y_test= lb.transform(y_test)\n",
    "\n",
    "    #Y_train = to_categorical(train_outputs)\n",
    "    #Y_test = to_categorical(test_outputs)\n",
    "\n",
    "    return X_train_sc, Y_train, X_test_sc, Y_test, lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model and search space for the optimisation algorithm\n",
    "def model(X_train_sc, Y_train, X_test_sc, Y_test):\n",
    "    \n",
    "    #define ANN model and search space\n",
    "    def ANN():\n",
    "        \n",
    "        #define first two layers, possible alternatives for neurons in each,\n",
    "        #activation function, and dropout layers\n",
    "        model=Sequential()\n",
    "        model.add(Dense({{choice([8,16, 24, 32, 64])}}))\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid', 'tanh'])}}))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 0.3)}}))\n",
    "\n",
    "        model.add(Dense({{choice([8,16, 24, 32, 64])}}))\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid', 'tanh'])}}))\n",
    "\n",
    "        model.add(Dropout({{uniform(0, 0.3)}}))\n",
    "        \n",
    "        #define output layer of the model\n",
    "        \n",
    "        model.add(Dense(2))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        #define optimisation algorithm for network training\n",
    "        optim=tf.keras.optimizers.Adam(learning_rate={{choice([0.005, 0.001])}})\n",
    "        \n",
    "        #compile model and return it\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    #call the ANN and ddefine training epochs; define batch size alternatives\n",
    "    net = KerasClassifier(build_fn = ANN,\n",
    "                                 epochs={{choice([50,100,200])}},\n",
    "                                 batch_size= {{choice([32,64])}},\n",
    "                                 verbose = 0)\n",
    "    model = ANN()\n",
    "    \n",
    "    #set up cross-validation scoring, and returned variables\n",
    "    c = cross_val_score(net,\n",
    "                    X_train_sc, Y_train,\n",
    "                    cv= StratifiedKFold(n_splits=5, shuffle=True),\n",
    "                    scoring='f1').mean()\n",
    "    print('Test accuracy:', c)\n",
    "    return {'loss': -c, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call in data function for test evaluation later\n",
    "X_train, Y_train, X_test, Y_test, lb = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the optimisation algorithm\n",
    "best_run, best_model = optim.minimize(model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='ANN_Optimisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the parameters for the best performing model\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model to gt learning curves\n",
    "batch=[32,64]\n",
    "ep=[50,100,200]\n",
    "history = best_model.fit(\n",
    "    X_train, Y_train,\n",
    "    batch_size=batch[best_run['batch_size']],\n",
    "    epochs=ep[best_run['epochs']],\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the labels for the test set\n",
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate test performance of the model\n",
    "print(confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print(classification_report(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Visualize history\n",
    "# Plot history: Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation accuracy history')\n",
    "plt.ylabel('Accuracy value (%)')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cancer is encoded by',lb.transform(['Cancer']), ', while Control is encoded by',lb.transform(['Control']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
